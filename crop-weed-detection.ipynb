{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8135826,"sourceType":"datasetVersion","datasetId":4809430},{"sourceId":8143329,"sourceType":"datasetVersion","datasetId":4814989}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchvision\n!apt-get update && apt-get install libgl1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-17T07:35:09.799794Z","iopub.execute_input":"2024-04-17T07:35:09.800322Z","iopub.status.idle":"2024-04-17T07:35:29.249902Z","shell.execute_reply.started":"2024-04-17T07:35:09.800288Z","shell.execute_reply":"2024-04-17T07:35:29.248701Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.1.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision) (1.3.0)\nGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\nGet:2 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1225 B]   \nGet:3 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]       \nGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\nHit:5 http://archive.ubuntu.com/ubuntu focal InRelease\nGet:6 https://packages.cloud.google.com/apt google-fast-socket InRelease [5015 B]\nGet:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\nGet:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3549 kB]\nGet:9 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1197 kB]\nGet:10 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.8 kB]\nGet:11 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3490 kB]\nGet:12 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [627 kB]\nHit:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease              \nGet:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1500 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1493 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.5 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3639 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4024 kB]\nFetched 19.8 MB in 2s (9215 kB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nlibgl1 is already the newest version (1.3.2-1~ubuntu0.20.04.2).\nlibgl1 set to manually installed.\n0 upgraded, 0 newly installed, 0 to remove and 69 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport numpy as np\nimport argparse\nfrom pathlib import Path\nfrom sklearn.model_selection import KFold\nimport math \nimport kornia\nimport logging\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport random\nimport os\nimport optuna\nimport cv2\nimport albumentations as A\nimport torch.nn as nn\nfrom albumentations.pytorch import ToTensorV2\nfrom collections import OrderedDict\nfrom torchvision._internally_replaced_utils import load_state_dict_from_url","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:00.885004Z","iopub.execute_input":"2024-04-17T07:35:00.885282Z","iopub.status.idle":"2024-04-17T07:35:09.798032Z","shell.execute_reply.started":"2024-04-17T07:35:00.885257Z","shell.execute_reply":"2024-04-17T07:35:09.797200Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# from utils.manual_fcn import load_fcn_resnet\n# from utils.manual_unet import UNet\n# from utils.manual_dlplus import DLv3plus\n# ","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.251329Z","iopub.execute_input":"2024-04-17T07:35:29.251623Z","iopub.status.idle":"2024-04-17T07:35:29.256384Z","shell.execute_reply.started":"2024-04-17T07:35:29.251597Z","shell.execute_reply":"2024-04-17T07:35:29.255231Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class UAVDatasetPatches(Dataset):\n    def __init__(self, img_list, msk_list, transform=None):\n        '''\n        img_ls: list of image Paths to load\n        msk_ls: list of mask Paths to load\n        loads the dataset from a list of images and masks\n        '''\n        self.transform = transform\n        self.img_list= img_list\n        self.msk_list= msk_list\n        assert len(self.img_list) == len(self.msk_list), \"Image and Mask Patches have different lengths.\"\n    def __len__(self):\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        image = cv2.imread(str(self.img_list[idx]))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(str(self.msk_list[idx]), cv2.IMREAD_GRAYSCALE) # load as np.float32\n        if self.transform is not None:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.259311Z","iopub.execute_input":"2024-04-17T07:35:29.259691Z","iopub.status.idle":"2024-04-17T07:35:29.355843Z","shell.execute_reply.started":"2024-04-17T07:35:29.259658Z","shell.execute_reply":"2024-04-17T07:35:29.354843Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_calculated_means_stds_per_fold(fold):\n    means = [\n        [0.4895940504368177, 0.4747875829353402, 0.42545172025367883],\n        [0.4909516814094245, 0.47507395584447076, 0.4252166750637278],\n        [0.4863172918463077, 0.4720067749001233, 0.42307293323046524],\n        [0.48556443799258586, 0.471592906257259, 0.42337851381822833]\n        ]\n    stds = [\n        [0.1329905783602554, 0.130645279821384, 0.12234299715980072],\n        [0.12910633924968123, 0.12635436744763892, 0.1180632138245313],\n        [0.1329739900037901, 0.1304754029316029, 0.12181500603654097],\n        [0.1335583288658572, 0.1313047051909438, 0.12297522870807812]\n    ]\n    return means[fold], stds[fold]","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.356993Z","iopub.execute_input":"2024-04-17T07:35:29.357286Z","iopub.status.idle":"2024-04-17T07:35:29.368235Z","shell.execute_reply.started":"2024-04-17T07:35:29.357257Z","shell.execute_reply":"2024-04-17T07:35:29.367538Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\n\nclass FCNHead(nn.Sequential):\n    def __init__(self, in_ch, out_ch):\n        \"\"\"\n        Caution: we still use Dropout, even tho not in the original implementation\n        \"\"\"\n        mid_ch = in_ch // 4\n        layers = [\n            nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_ch),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(mid_ch, out_ch, kernel_size=1),\n        ]\n\n        super().__init__(*layers)\n\nclass FCNtv(nn.Module):\n    \"\"\"\n    FCN network without skip connections. \n    If there is no dilation in the backbone, the model will be equal to FCN32s.\n    The interpolation will make [B, C, 16, 16] --> [B, C, 256, 256]\n    If a dilation of x4 is used in the backbone, \n    then the model will be equal to FCN8s \n    without any skip connections between intermediate layers\n    The interpolation will make [B, C, 64, 64] --> [B, C, 256, 256]\n    Currently implemented is only bilinear interpolation. \n    \"\"\"\n    def __init__(self, encoder_name, backbone, head, num_classes=3, n_upsample=32, b_bilinear=True, replace_stride_with_dilation=False):\n        super().__init__()\n        self.encoder_name = \"resnet50\"\n        self.backbone = backbone\n        self.head = head\n        print(f\"using test {encoder_name}, {n_upsample}x upsampling with replace strides {replace_stride_with_dilation} and bilinear {b_bilinear}\")\n\n\n    def forward(self, x):\n        input_shape = x.shape[-2:]\n        x = self.backbone(x)[\"layer4\"]\n        x = self.head(x)\n\n        x = F.interpolate(x, size=input_shape, mode=\"bilinear\", align_corners=False)\n\n        return x\n\n\nclass FCNskip(nn.Module):\n\n    def __init__(self, encoder_name, backbone, head, num_classes=3, n_upsample=32, b_bilinear=True, replace_stride_with_dilation=False):\n        super().__init__()\n        self.n_upsample = n_upsample\n        self.encoder_name = encoder_name\n        self.replace_stride_with_dilation = replace_stride_with_dilation\n        self.b_bilinear=b_bilinear\n        print(f\"using {encoder_name}, {n_upsample}x upsampling with replace strides {replace_stride_with_dilation} and bilinear {b_bilinear}\")\n\n        self.bn = nn.BatchNorm2d(num_features=num_classes)\n        self.onebyone128 = nn.Conv2d(128, num_classes, kernel_size=1)\n        self.onebyone256 = nn.Conv2d(256, num_classes, kernel_size=1)\n        self.onebyone512 = nn.Conv2d(512, num_classes, kernel_size=1)\n        self.onebyone1024 = nn.Conv2d(1024, num_classes, kernel_size=1)\n        if not b_bilinear:\n            self.convTranspose = nn.ConvTranspose2d(in_channels=num_classes, out_channels=num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n        self.backbone = backbone\n\n\n        self.head = head\n\n    def forward(self, x):\n        input_shape = x.shape[-2:]\n        x = self.backbone(x)\n\n        layer4 = self.head(x[\"layer4\"])\n\n        if self.n_upsample == 16: # use FCN-16s model\n            # upsampling needs to be based on the num_classes feature maps\n\n            # get intermediate layer and match channels\n            if self.encoder_name in [\"resnet18\", \"resnet34\"]:\n                layer3 = self.onebyone256(x[\"layer3\"])\n            else:\n                layer3 = self.onebyone1024(x[\"layer3\"])\n            # upsample last layer 2x to match spatial resolution\n            if self.b_bilinear:\n                x = F.interpolate(layer4, scale_factor=2.0, mode=\"bilinear\", align_corners=False)\n            else:\n                x = self.convTranspose(layer4)\n            # concat both\n            x = self.bn(x + layer3)\n            # final upsampling: here x16. This was fixed in the original paper\n\n            x = F.interpolate(x, size=input_shape, mode=\"bilinear\", align_corners=False)\n        elif self.n_upsample == 8: # use FCN-8s model\n\n            # upsampling needs to be based on the num_classes feature maps\n\n            # get intermediate layer and match channels\n            if self.encoder_name in [\"resnet18\", \"resnet34\"]:\n                layer3 = self.onebyone256(x[\"layer3\"])\n                layer2 = self.onebyone128(x[\"layer2\"])\n            else:\n                layer3 = self.onebyone1024(x[\"layer3\"])\n                layer2 = self.onebyone512(x[\"layer2\"])\n            # upsample layer4 2x to match spatial resolution of layer3\n            if self.b_bilinear:\n                layer4 = F.interpolate(layer4, scale_factor=2.0, mode=\"bilinear\", align_corners=False)\n            else:\n                layer4 = self.convTranspose(layer4)\n            \n            # concat both\n            x = self.bn(layer3 + layer4)\n            # upsample result of layer4 + 3 to match spatial resolution of layer2\n            \n            if self.b_bilinear:\n                x = F.interpolate(x, scale_factor=2.0, mode=\"bilinear\", align_corners=False)\n            else:\n                x = self.convTranspose(x)\n            x = self.bn(x + layer2)\n            # final upsampling: here x8. This was fixed in the original paper\n            x = F.interpolate(x, size=input_shape, mode=\"bilinear\", align_corners=False)\n        else:\n            raise NotImplementedError(f\"Upsampling of {self.n_upsample} is not implemented. Use either, 8, 16 or 32\")\n        return x\n\n\n\ndef load_fcn_resnet(encoder_name, num_classes=3, pretrained = False, replace_stride_with_dilation=False, n_upsample=32, b_bilinear=True):\n    \"\"\"\n    Constructs a Fully-Convolutional Network model with a ResNet backbone.\n    \"\"\"\n\n    if encoder_name in [\"resnet18\", \"resnet34\"]:\n        head = FCNHead(512, num_classes)\n    else:\n        head = FCNHead(2048, num_classes)\n\n    if replace_stride_with_dilation:\n        backbone = load_resnet(\n            encoder_name=encoder_name, \n            num_classes=num_classes, \n            pretrained=pretrained, \n            replace_stride_with_dilation=True\n            )\n        # set n_upsample =8, if we use dilated convolutions in the feature extractor --> no skip connections needed\n        if n_upsample == 8:\n            fcn = FCNtv(encoder_name, backbone, head, num_classes=num_classes, n_upsample=n_upsample, b_bilinear=b_bilinear, replace_stride_with_dilation=True)\n        else:\n            raise NotImplementedError(f\"upsampling of {n_upsample} not implemented when using dilation instead of stride\")\n    else:\n        backbone = load_resnet(\n            encoder_name=encoder_name, \n            num_classes=num_classes, \n            pretrained=pretrained, \n            replace_stride_with_dilation=False\n            )\n        if n_upsample in [8, 16]:\n            fcn = FCNskip(encoder_name, backbone, head, num_classes=num_classes, n_upsample=n_upsample, b_bilinear=b_bilinear, replace_stride_with_dilation=False)\n        elif n_upsample == 32:\n            fcn = FCNtv(encoder_name, backbone, head, num_classes=num_classes, n_upsample=n_upsample, b_bilinear=b_bilinear, replace_stride_with_dilation=False)\n        else:\n            raise NotImplementedError(f\"upsampling of {n_upsample} not implemented when not using dilation\")\n\n    return fcn","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.369513Z","iopub.execute_input":"2024-04-17T07:35:29.369873Z","iopub.status.idle":"2024-04-17T07:35:29.401716Z","shell.execute_reply.started":"2024-04-17T07:35:29.369844Z","shell.execute_reply":"2024-04-17T07:35:29.400868Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',\n    }\n\ndef replace_strides_with_dilation(module, dilation_rate):\n    \"\"\"Patch Conv2d modules replacing strides with dilation\"\"\"\n    for mod in module.modules():\n        if isinstance(mod, nn.Conv2d):\n            mod.stride = (1, 1)\n            mod.dilation = (dilation_rate, dilation_rate)\n            kh, _ = mod.kernel_size\n            mod.padding = ((kh // 2) * dilation_rate, (kh // 2) * dilation_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.402817Z","iopub.execute_input":"2024-04-17T07:35:29.403172Z","iopub.status.idle":"2024-04-17T07:35:29.416013Z","shell.execute_reply.started":"2024-04-17T07:35:29.403135Z","shell.execute_reply":"2024-04-17T07:35:29.415048Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1, padding=1, dilation=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=stride,\n                     padding=padding, dilation=dilation, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, stride=1,\n                     padding=1, dilation=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_ch)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_ch, mid_ch, out_ch, stride=1, padding=1, dilation=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_ch, out_channels=mid_ch, kernel_size=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_ch)\n        self.conv2 = nn.Conv2d(in_channels=mid_ch, out_channels=mid_ch, kernel_size=3, stride=stride, padding=padding, dilation=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_ch)\n        self.conv3 = nn.Conv2d(in_channels=mid_ch, out_channels=out_ch, kernel_size=1, stride=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.417524Z","iopub.execute_input":"2024-04-17T07:35:29.417912Z","iopub.status.idle":"2024-04-17T07:35:29.433132Z","shell.execute_reply.started":"2024-04-17T07:35:29.417880Z","shell.execute_reply":"2024-04-17T07:35:29.432234Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\nclass ResNet18(nn.Module):\n    def __init__(self, num_classes=3, output_stride=32):\n        super().__init__()\n        self.num_classes = num_classes\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        #first basic block\n        layers = [BasicBlock(in_ch=64, out_ch=64, stride=1, downsample=None),\n                  BasicBlock(in_ch=64, out_ch=64, stride=1, downsample=None)\n                  ]\n        self.layer1 = nn.Sequential(*layers)\n        #second basic block, here we need also a \"downsample\" sequential\n        layers = [nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(128)\n                  ]\n        downsample = nn.Sequential(*layers)\n        \n        layers = [BasicBlock(in_ch=64, out_ch=128, stride=2, downsample=downsample),\n                  BasicBlock(in_ch=128, out_ch=128, stride=1, downsample=None)]\n\n        self.layer2 = nn.Sequential(*layers)\n\n        # third basic block, here we need also a \"downsample\" sequential\n\n        layers = [nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(256)\n                  ]\n\n        downsample = nn.Sequential(*layers)\n        strides = [2, 1]\n        paddings = [1, 1]\n        dilations = [1, 1]\n\n        layers = [BasicBlock(in_ch=128, out_ch=256, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  BasicBlock(in_ch=256, out_ch=256, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None)]\n\n        self.layer3 = nn.Sequential(*layers)\n\n        # fourth basic block, here we need also a \"downsample\" sequential\n        layers = [nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(512)\n                  ]\n        downsample = nn.Sequential(*layers)\n        strides = [2, 1]\n        paddings = [1, 1]\n        dilations = [1, 1]\n\n        layers = [BasicBlock(in_ch=256, out_ch=512, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  BasicBlock(in_ch=512, out_ch=512, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None)]\n\n        self.layer4 = nn.Sequential(*layers)\n        if output_stride !=32:\n            self.make_dilated(output_stride=output_stride)\n\n    def forward(self, x):\n        layers = OrderedDict()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        layers[\"layer0\"]=x\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        layers[\"layer1\"]=x\n        x = self.layer2(x)\n        layers[\"layer2\"]=x\n        x = self.layer3(x)\n        layers[\"layer3\"]=x\n        x = self.layer4(x)\n        layers[\"layer4\"]=x\n        return layers\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n    \n    def make_dilated(self, output_stride):\n        print(f\"making dilated model\")\n        if output_stride == 16:\n            stage_list=[5,]\n            dilation_list=[2,]\n            \n        elif output_stride == 8:\n            stage_list=[4, 5]\n            dilation_list=[2, 4] \n\n        else:\n            raise ValueError(\"Output stride should be 16 or 8, got {}.\".format(output_stride))\n        \n        stages = self.get_stages()\n        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n            replace_strides_with_dilation(\n                module=stages[stage_indx],\n                dilation_rate=dilation_rate,\n            )\n\nclass ResNet34(nn.Module):\n    def __init__(self, num_classes=3, output_stride=32):\n        super().__init__()\n        self.num_classes = num_classes\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        #first basic block\n        layers = [BasicBlock(in_ch=64, out_ch=64, stride=1, downsample=None),\n                  BasicBlock(in_ch=64, out_ch=64, stride=1, downsample=None),\n                  BasicBlock(in_ch=64, out_ch=64, stride=1, downsample=None)\n                  ]\n        self.layer1 = nn.Sequential(*layers)\n        #second basic block, here we need also a \"downsample\" sequential\n        layers = [nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(128)\n                  ]\n        downsample = nn.Sequential(*layers)\n        \n        layers = [BasicBlock(in_ch=64, out_ch=128, stride=2, downsample=downsample),\n                  BasicBlock(in_ch=128, out_ch=128, stride=1, downsample=None),\n                  BasicBlock(in_ch=128, out_ch=128, stride=1, downsample=None),\n                  BasicBlock(in_ch=128, out_ch=128, stride=1, downsample=None)]\n\n        self.layer2 = nn.Sequential(*layers)\n\n        # third basic block, here we need also a \"downsample\" sequential\n        layers = [nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(256)\n                  ]\n        downsample = nn.Sequential(*layers)\n        \n        strides = [2, 1, 1, 1, 1, 1]\n        paddings = [1, 1, 1, 1, 1, 1]\n        dilations = [1, 1, 1, 1, 1, 1]\n\n        layers = [BasicBlock(in_ch=128, out_ch=256, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  BasicBlock(in_ch=256, out_ch=256, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None),\n                  BasicBlock(in_ch=256, out_ch=256, stride=strides[2], padding=paddings[2], dilation=dilations[2], downsample=None),\n                  BasicBlock(in_ch=256, out_ch=256, stride=strides[3], padding=paddings[3], dilation=dilations[3], downsample=None),\n                  BasicBlock(in_ch=256, out_ch=256, stride=strides[4], padding=paddings[4], dilation=dilations[4], downsample=None),\n                  BasicBlock(in_ch=256, out_ch=256, stride=strides[5], padding=paddings[5], dilation=dilations[5], downsample=None)]\n\n        self.layer3 = nn.Sequential(*layers)\n\n        # fourth basic block, here we need also a \"downsample\" sequential\n        layers = [nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(512)\n                  ]\n        downsample = nn.Sequential(*layers)\n        \n\n        strides = [2, 1, 1]\n        paddings = [1, 1, 1]\n        dilations = [1, 1, 1]\n\n        layers = [BasicBlock(in_ch=256, out_ch=512, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  BasicBlock(in_ch=512, out_ch=512, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None),\n                  BasicBlock(in_ch=512, out_ch=512, stride=strides[2], padding=paddings[2], dilation=dilations[2], downsample=None)]\n\n        self.layer4 = nn.Sequential(*layers)\n        if output_stride !=32:\n            self.make_dilated(output_stride=output_stride)\n\n    def forward(self, x):\n        layers = OrderedDict()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        layers[\"layer0\"]=x\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        layers[\"layer1\"]=x\n        x = self.layer2(x)\n        layers[\"layer2\"]=x\n        x = self.layer3(x)\n        layers[\"layer3\"]=x\n        x = self.layer4(x)\n        layers[\"layer4\"]=x\n        return layers\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n    \n    def make_dilated(self, output_stride):\n        print(f\"making dilated model\")\n        if output_stride == 16:\n            stage_list=[5,]\n            dilation_list=[2,]\n            \n        elif output_stride == 8:\n            stage_list=[4, 5]\n            dilation_list=[2, 4] \n\n        else:\n            raise ValueError(\"Output stride should be 16 or 8, got {}.\".format(output_stride))\n        \n        stages = self.get_stages()\n        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n            replace_strides_with_dilation(\n                module=stages[stage_indx],\n                dilation_rate=dilation_rate,\n            )\n\nclass ResNet50(nn.Module):\n    \"\"\"\n    check if we can simplify the replace_stride_with_dilation if statement\n    \"\"\"\n    def __init__(self, num_classes=3, output_stride=32):\n        super().__init__()\n        self.num_classes = num_classes\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        #first Bottleneck block\n        layers = [nn.Conv2d(in_channels=64, out_channels=256, kernel_size=1, stride=1, bias=False), \n                  nn.BatchNorm2d(256)\n                  ]\n        downsample = nn.Sequential(*layers)\n\n        layers = [Bottleneck(in_ch=64, mid_ch=64, out_ch=256, stride=1, downsample=downsample),\n                  Bottleneck(in_ch=256, mid_ch=64, out_ch=256, stride=1, downsample=None),\n                  Bottleneck(in_ch=256, mid_ch=64, out_ch=256, stride=1, downsample=None)\n                  ]\n        self.layer1 = nn.Sequential(*layers)\n\n\n        #second Bottleneck block\n        layers = [nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(512)\n                  ]\n        downsample = nn.Sequential(*layers)\n\n        layers = [Bottleneck(in_ch=256, mid_ch=128, out_ch=512, stride=2, downsample=downsample),\n                  Bottleneck(in_ch=512, mid_ch=128, out_ch=512, stride=1, downsample=None),\n                  Bottleneck(in_ch=512, mid_ch=128, out_ch=512, stride=1, downsample=None),\n                  Bottleneck(in_ch=512, mid_ch=128, out_ch=512, stride=1, downsample=None),\n                  ]\n        self.layer2 = nn.Sequential(*layers)\n\n        # third Bottleneck block\n        layers = [nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(1024)\n                  ]\n        \n        downsample = nn.Sequential(*layers)\n        strides = [2, 1, 1, 1, 1, 1]\n        paddings = [1, 1, 1, 1, 1, 1]\n        dilations = [1, 1, 1, 1, 1, 1]\n\n\n        layers = [Bottleneck(in_ch=512, mid_ch=256, out_ch=1024, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[2], padding=paddings[2], dilation=dilations[2], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[3], padding=paddings[3], dilation=dilations[3], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[4], padding=paddings[4], dilation=dilations[4], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[5], padding=paddings[5], dilation=dilations[5], downsample=None),\n                  ]\n\n        self.layer3 = nn.Sequential(*layers)\n\n        # fourth Bottleneck block\n\n        layers = [nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(2048)\n                  ]\n\n        downsample = nn.Sequential(*layers)\n\n        strides = [2, 1, 1]\n        paddings = [1, 1, 1]\n        dilations = [1, 1, 1]\n\n        layers = [Bottleneck(in_ch=1024, mid_ch=512, out_ch=2048, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  Bottleneck(in_ch=2048, mid_ch=512, out_ch=2048, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None),\n                  Bottleneck(in_ch=2048, mid_ch=512, out_ch=2048, stride=strides[2], padding=paddings[2], dilation=dilations[2], downsample=None),\n                  ]\n\n        self.layer4 = nn.Sequential(*layers)\n        if output_stride !=32:\n            self.make_dilated(output_stride=output_stride)\n\n    def forward(self, x):\n        layers = OrderedDict()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        layers[\"layer0\"]=x\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        layers[\"layer1\"]=x\n        x = self.layer2(x)\n        layers[\"layer2\"]=x\n        x = self.layer3(x)\n        layers[\"layer3\"]=x\n        x = self.layer4(x)\n        layers[\"layer4\"]=x\n\n        return layers\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n    \n    def make_dilated(self, output_stride):\n        print(f\"making dilated model\")\n        if output_stride == 16:\n            stage_list=[5,]\n            dilation_list=[2,]\n            \n        elif output_stride == 8:\n            stage_list=[4, 5]\n            dilation_list=[2, 4] \n\n        else:\n            raise ValueError(\"Output stride should be 16 or 8, got {}.\".format(output_stride))\n        \n        stages = self.get_stages()\n        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n            replace_strides_with_dilation(\n                module=stages[stage_indx],\n                dilation_rate=dilation_rate,\n            )\n\n\nclass ResNet101(nn.Module):\n    \"\"\"\n    check if we can simplify the replace_stride_with_dilation if statement\n    \"\"\"\n    def __init__(self, num_classes=3, output_stride=32):\n        super().__init__()\n        self.num_classes = num_classes\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        #first Bottleneck block\n        layers = [nn.Conv2d(in_channels=64, out_channels=256, kernel_size=1, stride=1, bias=False), \n                  nn.BatchNorm2d(256)\n                  ]\n        downsample = nn.Sequential(*layers)\n\n        layers = [Bottleneck(in_ch=64, mid_ch=64, out_ch=256, stride=1, downsample=downsample),\n                  Bottleneck(in_ch=256, mid_ch=64, out_ch=256, stride=1, downsample=None),\n                  Bottleneck(in_ch=256, mid_ch=64, out_ch=256, stride=1, downsample=None)\n                  ]\n        self.layer1 = nn.Sequential(*layers)\n\n\n        #second Bottleneck block\n        layers = [nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(512)\n                  ]\n        downsample = nn.Sequential(*layers)\n\n        layers = [Bottleneck(in_ch=256, mid_ch=128, out_ch=512, stride=2, downsample=downsample),\n                  Bottleneck(in_ch=512, mid_ch=128, out_ch=512, stride=1, downsample=None),\n                  Bottleneck(in_ch=512, mid_ch=128, out_ch=512, stride=1, downsample=None),\n                  Bottleneck(in_ch=512, mid_ch=128, out_ch=512, stride=1, downsample=None),\n                  ]\n        self.layer2 = nn.Sequential(*layers)\n\n        # third Bottleneck block\n        layers = [nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(1024)\n                  ]\n        \n        downsample = nn.Sequential(*layers)\n        strides = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        paddings = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        dilations = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n\n        layers = [Bottleneck(in_ch=512, mid_ch=256, out_ch=1024, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[2], padding=paddings[2], dilation=dilations[2], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[3], padding=paddings[3], dilation=dilations[3], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[4], padding=paddings[4], dilation=dilations[4], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[5], padding=paddings[5], dilation=dilations[5], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[6], padding=paddings[6], dilation=dilations[6], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[7], padding=paddings[7], dilation=dilations[7], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[8], padding=paddings[8], dilation=dilations[8], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[9], padding=paddings[9], dilation=dilations[9], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[10], padding=paddings[10], dilation=dilations[10], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[11], padding=paddings[11], dilation=dilations[11], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[12], padding=paddings[12], dilation=dilations[12], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[13], padding=paddings[13], dilation=dilations[13], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[14], padding=paddings[14], dilation=dilations[14], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[15], padding=paddings[15], dilation=dilations[15], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[16], padding=paddings[16], dilation=dilations[16], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[17], padding=paddings[17], dilation=dilations[17], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[18], padding=paddings[18], dilation=dilations[18], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[19], padding=paddings[19], dilation=dilations[19], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[20], padding=paddings[20], dilation=dilations[20], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[21], padding=paddings[21], dilation=dilations[21], downsample=None),\n                  Bottleneck(in_ch=1024, mid_ch=256, out_ch=1024, stride=strides[22], padding=paddings[22], dilation=dilations[22], downsample=None),\n                  ]\n\n        self.layer3 = nn.Sequential(*layers)\n\n        # fourth Bottleneck block\n        layers = [nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=1, stride=2, bias=False), \n                  nn.BatchNorm2d(2048)\n                  ]\n\n        downsample = nn.Sequential(*layers)\n\n        strides = [2, 1, 1]\n        paddings = [1, 1, 1]\n        dilations = [1, 1, 1]\n\n        layers = [Bottleneck(in_ch=1024, mid_ch=512, out_ch=2048, stride=strides[0], padding=paddings[0], dilation=dilations[0], downsample=downsample),\n                  Bottleneck(in_ch=2048, mid_ch=512, out_ch=2048, stride=strides[1], padding=paddings[1], dilation=dilations[1], downsample=None),\n                  Bottleneck(in_ch=2048, mid_ch=512, out_ch=2048, stride=strides[2], padding=paddings[2], dilation=dilations[2], downsample=None),\n                  ]\n\n        self.layer4 = nn.Sequential(*layers)\n        if output_stride !=32:\n            self.make_dilated(output_stride=output_stride)\n\n    def forward(self, x):\n        layers = OrderedDict()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        layers[\"layer0\"]=x\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        layers[\"layer1\"]=x\n        x = self.layer2(x)\n        layers[\"layer2\"]=x\n        x = self.layer3(x)\n        layers[\"layer3\"]=x\n        x = self.layer4(x)\n        layers[\"layer4\"]=x\n\n        return layers\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            nn.Sequential(self.conv1, self.bn1, self.relu),\n            nn.Sequential(self.maxpool, self.layer1),\n            self.layer2,\n            self.layer3,\n            self.layer4,\n        ]\n    \n    def make_dilated(self, output_stride):\n        print(f\"making dilated model\")\n        if output_stride == 16:\n            stage_list=[5,]\n            dilation_list=[2,]\n            \n        elif output_stride == 8:\n            stage_list=[4, 5]\n            dilation_list=[2, 4] \n\n        else:\n            raise ValueError(\"Output stride should be 16 or 8, got {}.\".format(output_stride))\n        \n        stages = self.get_stages()\n        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n            replace_strides_with_dilation(\n                module=stages[stage_indx],\n                dilation_rate=dilation_rate,\n            )","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.434750Z","iopub.execute_input":"2024-04-17T07:35:29.435091Z","iopub.status.idle":"2024-04-17T07:35:29.539907Z","shell.execute_reply.started":"2024-04-17T07:35:29.435062Z","shell.execute_reply":"2024-04-17T07:35:29.538842Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def load_resnet(encoder_name, num_classes, pretrained, replace_stride_with_dilation, progress=True):\n    if replace_stride_with_dilation:\n        print(f\"replacing stride with dilation\")\n        output_stride = 8\n    else:\n        output_stride = 32\n    if encoder_name == \"resnet18\":\n        model = ResNet18(num_classes=num_classes, output_stride=output_stride)\n        for param in model.parameters():\n            param.requires_grad = True\n    elif encoder_name == \"resnet34\":\n        model = ResNet34(num_classes=num_classes, output_stride=output_stride)\n        for param in model.parameters():\n            param.requires_grad = True\n    elif encoder_name == \"resnet50\":\n        model = ResNet50(num_classes=num_classes, output_stride=output_stride)\n        for param in model.parameters():\n            param.requires_grad = True\n    elif encoder_name == \"resnet101\":\n        model = ResNet101(num_classes=num_classes, output_stride=output_stride)\n        for param in model.parameters():\n            param.requires_grad = True\n    elif encoder_name == \"resnet152\":\n        raise NotImplementedError(f\"{encoder_name} is not implemented.\")\n    else:\n        raise NotImplementedError(f\"{encoder_name} is not implemented.\")\n\n    if pretrained:\n        print(\"LOADING PRETRAINED MODEL WEIGHTS FROM IMAGENET\")\n        # get the state dict from URL\n        state_dict = load_state_dict_from_url(model_urls[encoder_name],\n                                              progress=progress)\n        # we need to remove the keys for the fully connected layer, as we only need the feature extractor\n        entries_to_remove = ('fc.weight', 'fc.bias')\n        for k in entries_to_remove:\n            state_dict.pop(k, None)\n        # actually loading the weights to the model    \n        model.load_state_dict(state_dict) \n    else:\n        print(\"TRAINING WITH RANDOM INITIALIZED WEIGHTS\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.543184Z","iopub.execute_input":"2024-04-17T07:35:29.543493Z","iopub.status.idle":"2024-04-17T07:35:29.556447Z","shell.execute_reply.started":"2024-04-17T07:35:29.543469Z","shell.execute_reply":"2024-04-17T07:35:29.555492Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.557497Z","iopub.execute_input":"2024-04-17T07:35:29.557817Z","iopub.status.idle":"2024-04-17T07:35:29.622209Z","shell.execute_reply.started":"2024-04-17T07:35:29.557788Z","shell.execute_reply":"2024-04-17T07:35:29.621101Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_patch_lists(data_path, subset):\n    path = Path(f\"{data_path}/{subset}/patches\")\n    imgPaths = list(path.glob('./img/*.png'))\n    img_list = sorted(imgPaths)\n    annPaths = list(path.glob('./msk/*.png'))\n    msk_list = sorted(annPaths)\n    return img_list, msk_list ","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.623653Z","iopub.execute_input":"2024-04-17T07:35:29.624025Z","iopub.status.idle":"2024-04-17T07:35:29.637448Z","shell.execute_reply.started":"2024-04-17T07:35:29.623990Z","shell.execute_reply":"2024-04-17T07:35:29.636545Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def set_study(db_name, study_name,root_path, seed, b_clean_study=False):\n    '''\n    Creates a new study in a sqlite database located in ./results/\n    '''\n    sampler = optuna.samplers.TPESampler(seed=seed)\n    storage = optuna.storages.RDBStorage(f\"sqlite:///{root_path}/results/{db_name}.db\", heartbeat_interval=1)\n    if b_clean_study:\n        print(f\"CAUTION: Deleting existing trials in study {study_name}\")\n        optuna.delete_study(study_name=study_name, storage=f\"sqlite:///{root_path}/results/{db_name}.db\")\n        \n    study = optuna.create_study(storage=storage, study_name=study_name, sampler=sampler, direction=\"minimize\", load_if_exists=True)\n    return study\n\ndef seed_all(seed):\n    '''\n    sets the initial seed for numpy and pytorch to get reproducible results. \n    One still need to restart the kernel to get reproducible results, as discussed in:\n    https://stackoverflow.com/questions/32172054/how-can-i-retrieve-the-current-seed-of-numpys-random-number-generator\n    '''\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\ndef get_loaders(train_img_dir, train_msk_dir, valid_img_dir ,valid_msk_dir, mean, std, batch_size, num_workers=4, pin_memory=True):\n    train_transform = A.Compose(\n        [    \n            A.HorizontalFlip(),\n            A.VerticalFlip(),\n            A.CLAHE(),\n            A.RandomRotate90(),\n            A.Transpose(),\n            A.Normalize(\n                mean = mean,\n                std = std,\n                max_pixel_value=255.0\n            ),\n            ToTensorV2(),\n        ]\n    )\n    valid_transform = A.Compose(\n        [\n            A.Normalize(\n                mean = mean,\n                std = std,\n                max_pixel_value=255.0\n            ),\n            ToTensorV2(),\n        ]\n    )\n    train_ds = UAVDatasetPatches(img_list=train_img_dir, msk_list=train_msk_dir, transform=train_transform)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n    valid_ds = UAVDatasetPatches(img_list=valid_img_dir, msk_list=valid_msk_dir, transform=valid_transform)\n    valid_loader = DataLoader(valid_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n\n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.638666Z","iopub.execute_input":"2024-04-17T07:35:29.639090Z","iopub.status.idle":"2024-04-17T07:35:29.652774Z","shell.execute_reply.started":"2024-04-17T07:35:29.639059Z","shell.execute_reply":"2024-04-17T07:35:29.651911Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def set_model(architecture, encoder_name, pretrained, b_bilinear, replace_stride_with_dilation, num_classes=3):\n    model_name = f\"{architecture}_{encoder_name}\"\n    print(f\"MODEL NAME: {model_name}\")\n    if architecture == \"fcn32s\":\n        if replace_stride_with_dilation:\n            model=load_fcn_resnet(encoder_name, \n            num_classes=num_classes, \n            pretrained = pretrained, \n            replace_stride_with_dilation=replace_stride_with_dilation, \n            n_upsample=8, \n            b_bilinear=b_bilinear\n            )\n        else:\n            model=load_fcn_resnet(encoder_name, \n            num_classes=num_classes, \n            pretrained = pretrained, \n            replace_stride_with_dilation=replace_stride_with_dilation, \n            n_upsample=32, \n            b_bilinear=b_bilinear\n            )\n\n    elif architecture == \"fcn16s\":\n        model=load_fcn_resnet(encoder_name, \n        num_classes=num_classes, \n        pretrained = pretrained, \n        replace_stride_with_dilation=replace_stride_with_dilation, \n        n_upsample=16, \n        b_bilinear=b_bilinear\n        )\n    elif architecture == \"fcn8s\":\n        model=load_fcn_resnet(encoder_name, \n        num_classes=num_classes, \n        pretrained = pretrained, \n        replace_stride_with_dilation=replace_stride_with_dilation, \n        n_upsample=8, \n        b_bilinear=b_bilinear\n        )\n    else:\n        raise NotImplementedError(\"Specified Model is not defined. Currently implemented architectures are: fcn. Currently implemented feature extractors: resnet50, resnet101\")\n    return model\n\ndef save_checkpoint(state, filename=\"my_ckpt.pth.tar\"):\n    torch.save(state, filename)\n    return\n\ndef train_epoch(loader, model, optimizer, loss_fn, scaler, trial_number=None, fold=None, cur_epoch=None):\n    with tqdm(loader, unit=\"batch\", leave=True) as tepoch:\n        losses = []\n        if fold is not None and trial_number is not None:\n            tepoch.set_description(f\"Training T{trial_number} F{fold} E{cur_epoch}\")\n        else:\n            tepoch.set_description(f\"Retraining E{cur_epoch}\")\n        for data, targets in tepoch:\n            data = data.float().to(device=DEVICE )\n            targets = targets.long().to(device=DEVICE)\n            # forward \n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():\n                with torch.set_grad_enabled(True):\n                    predictions = model(data)\n                    loss = loss_fn(predictions, targets)\n                # backward\n                \n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                # update loop\n                tepoch.set_postfix(train_loss=loss.item())\n                losses.append(loss.item())\n            tepoch.set_postfix(train_losses=np.array(losses).mean())\n    return loss.item()\n\ndef validate_epoch(loader, model, cur_epoch, fold=None, trial_number=None):\n    dice_loss = 0\n    predictions_whole = None \n    targets_whole = None \n    model.eval()\n    with torch.no_grad():\n        with tqdm(loader, unit=\"batch\", leave=False) as tepoch:\n            if fold is not None and trial_number is not None:\n                tepoch.set_description(f\"Validating T{trial_number} F{fold} E{cur_epoch}\")\n            else:\n                tepoch.set_description(f\"Validating E{cur_epoch}\")\n            for idx, (inputs, targets) in enumerate(tepoch):\n                inputs = inputs.float().to(device=DEVICE)\n                targets = targets.long().to(device=DEVICE)\n                predictions = model(inputs)\n                if predictions_whole is None:\n                    predictions_whole = predictions\n                else:\n                    predictions_whole = torch.cat((predictions_whole, predictions), dim=0)\n                if targets_whole is None:\n                    targets_whole = targets\n                else:\n                    targets_whole = torch.cat((targets_whole, targets), dim=0)\n                \n                \n            dice_loss = kornia.losses.dice_loss(predictions_whole, targets_whole).item()\n    logging.info(f\"Validating T{trial_number} F{fold} E{cur_epoch}: valid loss {dice_loss}\")\n    model.train()\n    return dice_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.653874Z","iopub.execute_input":"2024-04-17T07:35:29.654150Z","iopub.status.idle":"2024-04-17T07:35:29.675457Z","shell.execute_reply.started":"2024-04-17T07:35:29.654128Z","shell.execute_reply":"2024-04-17T07:35:29.674634Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    epochs_no_improve:int = 0\n    kfold = KFold(n_splits=num_folds, shuffle=False)\n    loss_total = np.ones(num_folds)*99999\n    epochs = np.ones(num_folds)*0\n    img_list, msk_list = get_patch_lists(data_path=data_path,subset=\"trainval\")\n    for fold, (train_ids, val_ids) in enumerate(kfold.split(img_list)):\n        train_img_dir = [img_list[i] for i in train_ids]\n        train_msk_dir = [msk_list[i] for i in train_ids]\n        valid_img_dir = [img_list[i] for i in val_ids]\n        valid_msk_dir = [msk_list[i] for i in val_ids]\n        epochs_no_improve = 0\n\n        model = set_model(architecture=architecture, encoder_name=encoder_name, pretrained=pretrained, b_bilinear=b_bilinear, replace_stride_with_dilation=replace_stride_with_dilation, num_classes=3).to(device=device)\n        \n        loss_fn = kornia.losses.DiceLoss()\n        lr = trial.suggest_loguniform(\"lr\", lr_ranges[0], lr_ranges[1])\n        print(f\"suggested LR: {lr}\")\n        reduce_factor = trial.suggest_int(\"lr_factor\", int(lr_factor_ranges[0]*10), int(lr_factor_ranges[1]*10), step=int(lr_factor_ranges[2]*10))\n        reduce_factor = reduce_factor*0.1\n        optimizer = optim.Adam(model.parameters(), lr = lr)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=reduce_factor, min_lr=lr_ranges[0], patience=lr_scheduler_patience)\n        means, stds = get_calculated_means_stds_per_fold(fold)\n        train_loader, valid_loader = get_loaders(\n            train_img_dir = train_img_dir,\n            train_msk_dir = train_msk_dir,\n            valid_img_dir = valid_img_dir, \n            valid_msk_dir = valid_msk_dir,\n            mean = means,\n            std = stds,\n            batch_size = batch_size,\n            num_workers = num_workers,\n            pin_memory = False,\n        )\n        scaler = torch.cuda.amp.GradScaler()\n        for epoch in range(max_epochs):\n            train_loss = train_epoch(\n                train_loader, \n                model, \n                optimizer, \n                loss_fn, \n                scaler, \n                cur_epoch=epoch,\n                trial_number=trial.number,\n                fold=fold,\n                )\n            checkpoint = {\n                \"state_dict\": model.state_dict(),\n                \"optimizer\":optimizer.state_dict(),\n            }\n            \n            valid_loss = validate_epoch(\n                valid_loader, \n                model, \n                cur_epoch=epoch, \n                trial_number=trial.number,\n                fold=fold,\n                )\n            scheduler.step(valid_loss)\n            \n            if valid_loss < loss_total[fold]:\n                loss_total[fold] = valid_loss\n                if b_save_checkpoint:\n                    save_checkpoint(checkpoint, filename=f\"{str(model_path)}/{architecture}_{encoder_name}_dil{int(replace_stride_with_dilation)}_bilin{int(b_bilinear)}_pre{int(pretrained)}.pth.tar\")\n            else:\n                epochs_no_improve+=1\n            # sometimes it can happen, that valid_loss is nan --> cannot save nan to database, so we need to change it\n            if math.isnan(valid_loss):\n                valid_loss = 99999\n            \n            if epochs_no_improve >= es_patience:\n                print(f\"Early Stopping on epoch {epoch}\")\n                epochs[fold]=epoch\n                break\n\n    trial.set_user_attr('Valid loss per fold', list(loss_total))\n    trial.set_user_attr('root path', root_path)\n    trial.set_user_attr('architecture', architecture)\n    trial.set_user_attr('encoder_name', encoder_name)\n    trial.set_user_attr('batch_size', batch_size)\n    trial.set_user_attr('b_bilinear', b_bilinear)\n    trial.set_user_attr('pretrained', pretrained)\n    trial.set_user_attr('replace_stride', replace_stride_with_dilation)\n    trial.set_user_attr('final_epoch', list(epochs))\n    trial.set_user_attr('lr_scheduler_patience', lr_scheduler_patience)\n    print(f\"Validation loss per fold: {loss_total}\")  \n    return np.mean(loss_total)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:35:29.676853Z","iopub.execute_input":"2024-04-17T07:35:29.677135Z","iopub.status.idle":"2024-04-17T07:35:29.696125Z","shell.execute_reply.started":"2024-04-17T07:35:29.677113Z","shell.execute_reply":"2024-04-17T07:35:29.695203Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"run_prefix:str = True\nb_save_checkpoint:bool = True\npretrained:bool = True\nb_bilinear:bool = True\nreplace_stride_with_dilation:bool = False\nencoder_name:str = \"resnet101\"\narchitecture:str = \"fcn32s\"\nlr_ranges = [0.1, 0.9, 0.1]\nroot_path: str = \"/kaggle/working/\"\ndata_path = Path(\"/kaggle/input/crop-dataset\")\nnum_folds:int = 4\nbatch_size:int = 100\nn_trials:int = 50\ndb_name= \"\"\nstudy_name = \"\"\nif db_name == \"\":\n        db_name:str = f\"{run_prefix}_{architecture}_{encoder_name}_dil{int(replace_stride_with_dilation)}_bilin{int(b_bilinear)}_pre{int(pretrained)}\"\nif study_name == \"\":\n    study_name:str = f\"{architecture}_{encoder_name}_dil{int(replace_stride_with_dilation)}_bilin{int(b_bilinear)}_pre{int(pretrained)}\"\nlr_factor_ranges = [0.1, 0.9, 0.1]\nmax_epochs:int = 100\nes_patience:int = 10\nlr_scheduler_patience:int = 5\nseed:int = 42\n\ndevice: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nnum_workers: int = 2 if torch.cuda.is_available() else 0\n\nseed_all(seed=seed)\n\n# Create Paths\nmodel_path = Path(f'{root_path}/models/')\nmodel_path.mkdir(parents=True, exist_ok=True)\nresult_path = Path(f'{root_path}/results/')\nresult_path.mkdir(parents=True, exist_ok=True)\n\nstudy = set_study(db_name=db_name, study_name=study_name, root_path=root_path, seed=seed)\n\nstudy.optimize(lambda trial: objective(trial), n_trials=n_trials)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_calculated_means_stds_trainval():\n    means = [0.48810686542128406, 0.4733653049842984, 0.4242799605915251]\n    stds = [0.1321881434144248, 0.12971921686190743, 0.12131885037092494]\n    return means, stds","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:42:05.469049Z","iopub.execute_input":"2024-04-17T07:42:05.469438Z","iopub.status.idle":"2024-04-17T07:42:05.474453Z","shell.execute_reply.started":"2024-04-17T07:42:05.469398Z","shell.execute_reply":"2024-04-17T07:42:05.473464Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"seed_all(seed=42)\narchitecture:str = \"fcn32s\"\nencoder_name:str = \"resnet101\"\ndb_name = \"\"\nroot_path: str = Path(\"/kaggle/working/\")\nif db_name ==\"\":\n    db_name:str = f\"retrain_{architecture}_{encoder_name}\"\nelse:\n    db_name = db_name\nprint(f\"loaded db {db_name}\")\n    # Parameters\nmax_epochs = 100\nes_patience = 5\nloss_total = 1\nepochs_no_improve = 0\n# NEED TO CHANGE THIS LINE OF CODE TO RE-TRAIN DIFFERENT MODELS\nstudy_storage = f\"sqlite:////kaggle/input/fcn-study-db-params/True_fcn32s_resnet101_dil0_bilin1_pre1.db\"\nstudies = optuna.study.get_all_study_summaries(storage=study_storage)\nloaded_study = optuna.load_study(study_name=studies[0].study_name, storage=study_storage)\ntrial = loaded_study.best_trial\ndevice: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Loading Study: {studies[0].study_name} from {db_name}\")\nprint(f\"Best Trial:{trial.number}\")\nprint(trial)\n    \nmodel_path = Path(f'{root_path}/models/')\nmodel_path.mkdir(parents=True, exist_ok=True)\nresult_path = Path(f'{root_path}/results/')\nresult_path.mkdir(parents=True, exist_ok=True)\n\n# extract hyperparameters, feature extractor and architecture from best trial\nlr = trial.params[\"lr\"]\nlr_factor = trial.params[\"lr_factor\"]\nbatch_size=trial.user_attrs[\"batch_size\"]\nlr_scheduler_patience = trial.user_attrs[\"lr_scheduler_patience\"]\narchitecture = trial.user_attrs[\"architecture\"]\nencoder_name = trial.user_attrs[\"encoder_name\"]\npretrained = trial.user_attrs[\"pretrained\"]\nb_bilinear = trial.user_attrs[\"b_bilinear\"]\nreplace_stride_with_dilation = trial.user_attrs[\"replace_stride\"]\ndata_path = Path(\"/kaggle/input/crop-dataset\") \n\ntrain_img_dir, train_msk_dir = get_patch_lists(\ndata_path=data_path, \nsubset=\"trainval\")\n\nvalid_img_dir, valid_msk_dir = get_patch_lists(\ndata_path=data_path, \nsubset=\"test\")\n\n\nmodel_save_str = f\"model_{architecture}_{encoder_name}_dil{int(replace_stride_with_dilation)}_bilin{int(b_bilinear)}_retrained.pt\"\nmodel_save_path = Path(root_path) / \"models\" / model_save_str\nmodel = set_model(architecture=architecture, encoder_name=encoder_name, pretrained=pretrained, b_bilinear=b_bilinear, replace_stride_with_dilation=replace_stride_with_dilation, num_classes=3).to(device=device)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = lr)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_factor*0.1, min_lr=1e-6, patience=lr_scheduler_patience)\nmeans, stds = get_calculated_means_stds_trainval()       \n\ntrain_loader, _ = get_loaders(\n        train_img_dir = train_img_dir,\n        train_msk_dir = train_msk_dir,\n        valid_img_dir = valid_img_dir, \n        valid_msk_dir = valid_msk_dir,\n        mean = means,\n        std = stds,\n        batch_size = batch_size,\n        num_workers = 4,\n        pin_memory = True,\n    )\nscaler = torch.cuda.amp.GradScaler()\nfor epoch in range(max_epochs):\n    train_loss = train_epoch(\n        train_loader, \n        model, \n        optimizer, \n        loss_fn, \n        scaler, \n        cur_epoch=epoch\n        )\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n    }\n    scheduler.step(train_loss)\n    if train_loss < loss_total:\n        loss_total = train_loss\n        print(f\"Saving checkpoint in epoch {epoch}...\")\n        save_checkpoint(checkpoint, filename=f\"{str(model_save_path)}\")\n    else:\n        epochs_no_improve+=1\n        # sometimes it can happen, that test_loss is nan --> cannot save nan to database, so we need to change it\n    if math.isnan(train_loss):\n        train_loss = 99999\n    if epochs_no_improve >= es_patience:\n        print(f\"Early Stopping on epoch {epoch}\")\n        break\n    print(f\"Loss on Train set: {train_loss}\")\nprint(train_loss)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T07:46:04.241075Z","iopub.execute_input":"2024-04-17T07:46:04.241926Z","iopub.status.idle":"2024-04-17T07:47:44.321006Z","shell.execute_reply.started":"2024-04-17T07:46:04.241889Z","shell.execute_reply":"2024-04-17T07:47:44.319845Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"loaded db retrain_fcn32s_resnet101\nLoading Study: fcn32s_resnet101_dil0_bilin1_pre1 from retrain_fcn32s_resnet101\nBest Trial:38\nFrozenTrial(number=38, state=TrialState.COMPLETE, values=[74999.26223023795], datetime_start=datetime.datetime(2024, 4, 17, 4, 13, 57, 260461), datetime_complete=datetime.datetime(2024, 4, 17, 4, 27, 43, 861928), params={'lr': 0.10016018426773525, 'lr_factor': 7}, user_attrs={'Valid loss per fold': [99999.0, 99999.0, 0.048920951783657074, 99999.0], 'architecture': 'fcn32s', 'b_bilinear': True, 'batch_size': 100, 'encoder_name': 'resnet101', 'final_epoch': [9.0, 9.0, 10.0, 9.0], 'lr_scheduler_patience': 5, 'pretrained': True, 'replace_stride': False, 'root path': '/kaggle/working/'}, system_attrs={}, intermediate_values={}, distributions={'lr': FloatDistribution(high=0.9, log=True, low=0.1, step=None), 'lr_factor': IntDistribution(high=9, log=False, low=1, step=1)}, trial_id=39, value=None)\nMODEL NAME: fcn32s_resnet101\nLOADING PRETRAINED MODEL WEIGHTS FROM IMAGENET\nusing test resnet101, 32x upsampling with replace strides False and bilinear True\n","output_type":"stream"},{"name":"stderr","text":"Retraining E0: 100%|██████████| 22/22 [00:20<00:00,  1.10batch/s, train_losses=nan] \n","output_type":"stream"},{"name":"stdout","text":"Loss on Train set: 99999\n","output_type":"stream"},{"name":"stderr","text":"Retraining E1: 100%|██████████| 22/22 [00:19<00:00,  1.11batch/s, train_losses=nan]\n","output_type":"stream"},{"name":"stdout","text":"Loss on Train set: 99999\n","output_type":"stream"},{"name":"stderr","text":"Retraining E2: 100%|██████████| 22/22 [00:19<00:00,  1.11batch/s, train_losses=nan]\n","output_type":"stream"},{"name":"stdout","text":"Loss on Train set: 99999\n","output_type":"stream"},{"name":"stderr","text":"Retraining E3: 100%|██████████| 22/22 [00:19<00:00,  1.12batch/s, train_losses=nan]\n","output_type":"stream"},{"name":"stdout","text":"Loss on Train set: 99999\n","output_type":"stream"},{"name":"stderr","text":"Retraining E4: 100%|██████████| 22/22 [00:19<00:00,  1.12batch/s, train_losses=nan]","output_type":"stream"},{"name":"stdout","text":"Early Stopping on epoch 4\n99999\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}